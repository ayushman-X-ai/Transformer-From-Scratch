# ğŸ§  Transformer From Scratch (PyTorch)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen)

A complete **Transformer architecture built from scratch** using **PyTorch**, inspired by the paper  
ğŸ“œ [*Attention Is All You Need* (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762).

This project demonstrates a full, modular implementation of a Transformer â€” including **embeddings**, **positional encodings**, **multi-head attention**, **encoder-decoder stacks**, and **attention visualization** â€” entirely coded by hand.  

---

## ğŸš€ Features

âœ… Implemented **from scratch** using pure PyTorch  
âœ… Includes **Encoder** and **Decoder** architecture  
âœ… Supports **multi-head self-attention**  
âœ… Visualizes **attention maps** for interpretability  
âœ… Modular, well-documented code structure  
âœ… Fully customizable for any NLP or sequence modeling task  

---

## ğŸ—ï¸ Architecture Overview

The Transformer model follows the original *Attention Is All You Need* structure:

Input â†’ Embedding â†’ Positional Encoding â†’
[Encoder Blocks Ã— N] â†’ [Decoder Blocks Ã— N] â†’ Linear â†’ Output

### Components Implemented
- **Input Embedding**
- **Positional Encoding**
- **Layer Normalization**
- **Feed Forward Networks**
- **Multi-Head Attention**
- **Residual Connections**
- **Encoder & Decoder Modules**
- **Attention Visualization**

---

## ğŸ“‚ Project Structure

pytorch-transformer/
â”‚
â”œâ”€â”€ transformer/
â”‚ â”œâ”€â”€ embeddings.py
â”‚ â”œâ”€â”€ positional_encoding.py
â”‚ â”œâ”€â”€ attention.py
â”‚ â”œâ”€â”€ encoder.py
â”‚ â”œâ”€â”€ decoder.py
â”‚ â”œâ”€â”€ transformer.py
â”‚ â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ visualize_attention.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
